---
title:   |
  | HarvardX - Data Science Professional Certificate
  | Capstone Project
  | - MovieLens -
author:  "Anass Latif"
date:    "`r format(Sys.Date(), '%B %d, %Y')`"
output:  
   pdf_document:
      latex_engine: xelatex
      toc: true
      toc_depth: 2
      fig_width: 5
      fig_height: 3
      
fontsize: 11pt
geometry: margin=0.75in
urlcolor: blue
---


```{r 000-setup-prerequisites, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# =============================================================================
#     Step 000. Setup the prerequisites to generate the report as a PDF file 
# =============================================================================

knitr::opts_chunk$set(message = FALSE, 
                      echo = FALSE, 
                      fig.align = 'center')

```


```{r 001-configure-variables, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# =============================================================================
#     Step 001 - Configure all variables and session information
# =============================================================================

# Start the chronometer to get the script start time in milliseconds
script_start_time <- proc.time()

# Display the working directory
getwd()

# Display session information for reproductibility
sessionInfo()

# Use s seed number for reproductibility
set.seed(1)

# Setup RMSE target goal
RMSE_target_goal <- 0.87750

# Setup RMSE target goal
round_precision <- 5

```


```{r 002-install-packages, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# =============================================================================
#     Step 002 - Install all necessary packages if do not exist
# =============================================================================

# Package "tidyverse"
if(!require(tidyverse)) install.packages("tidyverse")

# Package "caret"
if(!require(caret)) install.packages("caret")

# Package "lubridate"
if(!require(lubridate)) install.packages("lubridate")

# Package "gridExtra"
if(!require(gridExtra)) install.packages("gridExtra")

# Package "ggrepel"
if(!require(ggrepel)) install.packages("ggrepel")

# Package "ggthemes"
if(!require(ggthemes)) install.packages("ggthemes")

# Package "knitr"
if(!require(knitr)) install.packages("knitr")

# Package "kableExtra"
if(!require(kableExtra)) install.packages("kableExtra")

# Package "summarytools"
if(!require(summarytools)) install.packages("summarytools")

```


```{r 003-load-libraries, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# =============================================================================
#     Step 003 - Load all necessary libraries
# =============================================================================

library(tidyverse)
library(caret)
library(lubridate)
library(gridExtra)
library(knitr)
library(kableExtra)
library(ggthemes)
library(ggrepel)
library(summarytools)

```

\newpage

<!-- ============================================================================== -->
<!--  Section  01. Introduction / Overview / Executive Summary                      -->
<!--              01.01. Background and Motivation                                  -->
<!--              01.02. Data Set                                                   -->
<!--              01.03. Goal                                                       -->
<!--              01.04. Key Steps                                                  -->
<!-- ============================================================================== -->
# 1. Introduction / Overview / Executive Summary


## Background and Motivation

**Recommendation systems** are one of the most famous machine learning models. They are extensively used by many companies (eg: Netflix, Amazon, Facebook, etc.) to improve and enhance user experiences and increase revenues by recommending the most relevant products to their customers.

This **Harvard Data Science Capstone** is the final assignment for [HarvardX - Data Science Professional Certificate](https://courses.edx.org/dashboard/programs/3c32e3e0-b6fe-4ee4-bd4f-210c6339e074/) from Harvard University. 

This project is motivated by the [Netflix challenge](https://netflixprize.com/index.html) that was organized in October, 2006. Netflix offered one million dollars reward to anyone that could improve their recommendation systems by 10%.


## Dataset

For this assignment, we will go throught all the steps to create a movie recommendation system using the MovieLens dataset, collected by [GroupLens Research](https://grouplens.org/) as the Netflix Datasets are private. 

We will be using the [10M](https://grouplens.org/datasets/movielens/10m/) version of the MovieLens dataset to make the computation a little easier.


## Goal

The objective of this report is to predict, in the most accurate and comprehensive way, the user **_movie ratings_** by implementing, testing and validating different machine learning models. 

The outcome is to provide a minimal typical error or RMSE (Root Mean Square Error) on the validation dataset with **_`RMSE lower or equal to 0.87750`_**.

The RMSE is defined as:

$$\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }$$

We define $y_{u,i}$ as the rating for movie $i$ by user $u$ and denote our prediction with $\hat{y}_{u,i}$ with $N$ being the number of user/movie combinations and the sum occurring over all these combinations.


## Key Steps

To achieve the project objectives, we will follow a comprehensive machine learning workflow:

1. Run the R code provided by Edx staff to generate the datasets. The script execute the following steps:
   * Download the MovieLens 10M dataset
   * Split the MovieLens dataset into training (`edx`) and test (`validation`) datasets.
2. Explore the `edx` dataset to discover the data and the available features. We will use some exploratory techniques such as data description, preparation, exploration and visualization.
3. Develop and train different predictive models and algorithms in order to find a recommendation model with the best possible outcome (RMSE) that meets our goals.
4. Explain the results and conclude.

All the project will be made through RStudio (version `r getRversion()`) using some useful packages (eg: dplyr, tidyverse, lubridate, caret, etc.). 

This report doesn't display the R code used to generate the information. All scripts are available in [My GitHub Repository](https://github.com/anass-latif/DataScience-HarvardX-MovieLens).

\newpage

<!-- ============================================================================== -->
<!--  Section  02. Second Section - Methods / Analysis                              -->
<!--              02.01. Data Preparation                                           -->
<!--              02.02. Exploratory Data Analysis                                  -->
<!--              02.03. Model Building, Training And Validation                    -->
<!-- ============================================================================== -->
# 2. Methods / Analysis


## Data Preparation


### Dataset Generation

```{r 004-generate-dataset, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# =============================================================================
#     Step 004 - Generate datasets using the original code provided by EDX Staff
# =============================================================================

#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>%
   semi_join(edx, by = "movieId") %>%
   semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

The generated datasets are divided into two subsets:

* a training subset to train our algorithm, called `edx`. This subset represents `90%` of the generated dataset.
* a validation subset to predict the movie ratings, called `validation`. This subset represents `10%` of the generated dataset.


### Dataset Description

```{r 005-01-edx-data-structure, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# =============================================================================
#     Step 005 - Understand the data structure
# =============================================================================

# 005-01 edx Dataset Structure
str(edx)
nrow(edx)
ncol(edx)
sum(is.na(edx))

```

The `edx` dataset contains **_`r nrow(edx)`_** observations (rows) of **_`r ncol(edx)`_** variables (columns). There is no missing values (**_`r sum(is.na(edx))`_** na).

```{r 005-02-validation-data-structure, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# 005-02 validation Dataset Structure
str(validation)
nrow(validation)
ncol(validation)
sum(is.na(validation))

```

The `validation` dataset contains **_`r nrow(validation)`_** observations (rows) of **_`r ncol(validation)`_** variables (columns). There is no missing values (**_`r sum(is.na(validation))`_** na).

The features / variables identified in both datasets are:

* `userId`: `integer` variable that represents the unique identification number for each user.
* `movieId`: `numeric` variable that represents the unique identification number for each movie.
* `rating`: `numeric` variable that represents the rating of one movie by one user. Ratings are made on a 5-star scale (0 to 5), with half-star increments.
* `timestamp`: `integer` variable that represents the number of seconds since midnight UTC (1970-JAN-01).
* `title`: `character` variable that represents the movie title including the year of the release.
* `genres`: `character` variable that represents a pipe-separated list of genres affected to each movie.

Let's display a sample of the `edx` dataset.

```{r 005-03-edx-dataset-sample}

# 005-03 edx Dataset Example
head(edx) %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)

```


### Dataset Preprocessing (Feature Selection and Engineering)

We notice that the `genres` are pipe-separated values. We might need this feature `movieGenre` to predict the movie rating. However, we should extract individual value for more consistent and robust estimate.

We also observe that the movie title contains the year where the movie was released. We might need this feature `releaseYear` to predict the movie rating.

In addition, we can extract the year where the movie was rated from `timestamp` to define the feature as `ratingYear`.

```{r 006-preprocess-dataset, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# =============================================================================
#     Step 006 - Preprocess the datasets
# =============================================================================

# 006-01 Create new movieGenre feature in a new dataset for both edx and validation datasets
movieGenre_edx <- edx %>%
   mutate(movieGenre = fct_explicit_na(genres,
                                       na_level = "(no genres listed)")
          ) %>%
   separate_rows(movieGenre,
                 sep = "\\|") %>%
   group_by(movieGenre) %>%
   summarize(Rating_Count = n(),
             Rating_Average = mean(rating),
             Movie_Distinct_Count = n_distinct(movieId),
             User_Distinct_Count = n_distinct(userId))


movieGenre_validation <- validation %>%
   mutate(movieGenre = fct_explicit_na(genres,
                                       na_level = "(no genres listed)")
          ) %>%
   separate_rows(movieGenre,
                 sep = "\\|") %>%
   group_by(movieGenre) %>%
   summarize(Rating_Count = n(),
             Rating_Average = mean(rating),
             Movie_Distinct_Count = n_distinct(movieId),
             User_Distinct_Count = n_distinct(userId))

# 006-02 Create new releaseYear + ratingYear features for both edx and validation datasets
#        Remove timestamp as is not used anymore
edx <- edx %>%
   mutate(title = str_trim(title)) %>%
   extract(title,
           c("titleTemporary", "releaseYear"),
           regex = "^(.*) \\(([0-9 \\-]*)\\)$",
           remove = F) %>%
   mutate(releaseYear = if_else(str_length(releaseYear) > 4,
                                as.integer(str_split(releaseYear, "-",
                                                     simplify = T)[1]),
                                as.integer(releaseYear))
   ) %>%
   mutate(title = if_else(is.na(titleTemporary),
                          title,
                          titleTemporary)
   ) %>%
   select(-titleTemporary) %>%
   mutate(ratingYear = year(as.POSIXct(timestamp,
                                       origin = "1970-01-01"))
   ) %>%
   select(-timestamp)

validation <- validation %>%
   mutate(title = str_trim(title)) %>%
   extract(title, c("titleTemporary", "releaseYear"),
           regex = "^(.*) \\(([0-9 \\-]*)\\)$",
           remove = F) %>%
   mutate(releaseYear = if_else(str_length(releaseYear) > 4,
                                as.integer(str_split(releaseYear, "-",
                                                     simplify = T)[1]),
                                as.integer(releaseYear))
   ) %>%
   mutate(title = if_else(is.na(titleTemporary),
                          title,
                          titleTemporary)
   ) %>%
   select(-titleTemporary) %>%
   mutate(ratingYear = year(as.POSIXct(timestamp,
                                       origin = "1970-01-01"))) %>%
   select(-timestamp)

```

After preprocessing the data, the `edx` dataset looks like:

```{r 006-03-edx-dataset-sample}

# 006-03 edx Dataset Example
head(edx) %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)

```

To summarize, the features that could be selected in the machine learning models to predict the `rating` are:

* `movieId`
* `userId`
* `movieGenre`
* `releaseYear`
* `ratingYear`

\newpage

## Exploratory Data Analysis (EDA)


The `edx` dataset contains **_`r n_distinct(edx$userId)`_** distinct users who rated **_`r n_distinct(edx$movieId)`_** distinct movies that are classified in **_`r n_distinct(movieGenre_edx$movieGenre)`_** distinct genres.

The `validation` dataset contains **_`r n_distinct(validation$userId)`_** distinct users who rated **_`r n_distinct(validation$movieId)`_** distinct movies that are classified in **_`r n_distinct(movieGenre_validation$movieGenre)`_** distinct genres.

Let's explore our `edx` dataset using some visualization techniques to build more comprehensive understanding of the data. So, here are some questions that we raise: 


### Rating Analysis


**_What is the overall ratings distribution?_**

```{r 007-01-WhatIsTheOverallRatingsDistribution}

# =============================================================================
#     Step 007 - Explore and visualize the data
# =============================================================================

# 007-01 What is the overall ratings distribution?
ratings_mu <- mean(edx$rating)

edx %>%
   ggplot(aes(rating)) +
   geom_histogram(color = "darkblue",
                  fill = "lightblue",
                  bins = 10) +
   scale_x_continuous(breaks = seq(0.5, 5, 0.5)) +
   geom_vline(xintercept = ratings_mu,
              col = "red",
              linetype = "dashed") +
   labs(title = "Overall Ratings Distribution (edx)",
        x = "Rating",
        y = "Frequency") +
   theme_economist() +
   scale_color_economist() +
   theme(plot.title = element_text(size = 11, color = "darkblue", hjust = 0.5))

```

The figure _"Overall Ratings Distribution (edx)"_ shows the rating distribution in the `edx` dataset. The `rating` variable has a left-skewed distribution which shows that there are more _"good"_ ratings than _"bad"_ ratings.

The vertical dashed line represents the overall rating average $\mu$ (**_`r ratings_mu`_**) across all users and all movies. We notice also that the rating range from 0.5 to 5 with the most common rating is 4.0 followed closely by 3.0. It also appears that we have a predilection for whole numbered ratings (full star rates) instead of half-star increment ratings.

\newpage

### Movie Analysis


**_How frequently are movies rated?_**

```{r 007-02-HowFrequentlyAreMoviesRated}

# 007-02 How frequently are movies rated?
edx %>%
   group_by(movieId) %>%
   summarise(Rating_Count = n()) %>%
   ggplot(aes(Rating_Count)) +
   geom_histogram(color = "darkblue",
                  fill = "lightblue",
                  bins = 50) +
   scale_x_log10() +
   labs(title = "Ratings Frequency Distribution Per Movie (edx)",
        x = "Movie Id",
        y = "Frequency") +
   theme_economist() +
   scale_color_economist() +
   theme(plot.title = element_text(size = 11, color = "darkblue", hjust = 0.5))

```

The figure _"Ratings Frequency Distribution Per Movie (edx)"_ shows that some movies are more popular than others, so they are much more rated.


**_What is the cumulative rating distribution by movie?_**

```{r 007-03-WhatIsTheCumulativeRatingDistributionByMovie}

# 007-03 What is the cumulative rating distribution by movie?
q90_movie <- edx %>%
   group_by(movieId) %>%
   summarise(Rating_Count = n())

quantile_movie <- round(quantile(q90_movie$Rating_Count, probs = 0.9), 0)

edx %>%
   group_by(movieId) %>%
   summarise(Rating_Count = n()) %>%
   ggplot(aes(Rating_Count)) +
   stat_ecdf(geom = "step",
             color = "darkblue") +
   geom_vline(xintercept = quantile_movie,
              col = "red",
              linetype = "dashed") +
   labs(title = "Cumulative Distribution ECDF Of Ratings Per Movie (edx)",
        x = "Movies",
        y = "Cumulative Proportion Of Ratings") +
   theme_economist() +
   scale_color_economist() +
   theme(plot.title = element_text(size = 11, color = "darkblue", hjust = 0.5))

```

The figure _"Cumulative Distribution ECDF Of Ratings Per Movie (edx)"_ shows that **_`r quantile_movie`_** distinct movies receive **_`90%`_** of the ratings.


\newpage

**_What are the top 10 rated movies?_**

```{r 007-04-WhatAreTheTop10RatedMovies}

# 007-04 What are the top 10 rated movies?
edx %>%
   group_by(movieId, title) %>%
   summarise(Rating_Count = n(),
             Rating_Average = round(mean(rating), round_precision)) %>%
   arrange(desc(Rating_Average)) %>%
   head(10) %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2, width = "20em") %>%
   column_spec(3:4, bold = TRUE)

```

The top 10 rated movies (based on the mean rating) had received only a few by exclusively excellent ratings. We should avoid ranking the movies if we have only few ratings. Arbitrarly, we will pick only the movies that have been rated more than 100 times.


**_What are the top 10 rated movies with at least 100 ratings?_**

```{r 007-05-WhatAreTheTop10RatedMoviesWithAtLeast100Ratings}

# 007-05 What are the top 10 rated movies with at least 100 ratings?
edx %>%
   group_by(movieId, title) %>%
   summarise(Rating_Count = n(),
             Rating_Average = round(mean(rating), round_precision)) %>%
   filter(Rating_Count >= 100) %>%
   arrange(desc(Rating_Average)) %>%
   head(10) %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2, width = "20em") %>%
   column_spec(3:4, bold = TRUE)

```

The table above shows that the top 10 rated movies are very famous. However, there is a substantial difference in the numbers of ratings that a specific movie receives.

\newpage

### User Analysis


**_How frequently do the users rate movies?_**

```{r 007-06-HowFrequentlyDoTheUsersRateMovies}

# 007-06 How frequently do the users rate movies?
edx %>%
   group_by(userId) %>%
   summarise(Rating_Count = n()) %>%
   ggplot(aes(Rating_Count)) +
   geom_histogram(color = "darkblue",
                  fill = "lightblue",
                  bins = 50) +
   scale_x_log10() +
   labs(title = "Ratings Frequency Distribution Per User (edx)",
        x = "Users",
        y = "Frequency") +
   theme_economist() +
   scale_color_economist() +
   theme(plot.title = element_text(size = 11, color = "darkblue", hjust = 0.5))

```

The figure _"Ratings Frequency Distribution Per User (edx)"_ shows that some users rate movies more often than others, so they are much more active.


**_What is the cumulative rating distribution by user?_**

```{r 007-07-WhatIsTheCumulativeRatingDistributionByUser}

# 007-07 What is the cumulative rating distribution by user?
q90_user <- edx %>%
   group_by(userId) %>%
   summarise(Rating_Count = n())

quantile_user <- round(quantile(q90_user$Rating_Count, probs = 0.9), 0)

edx %>%
   group_by(userId) %>%
   summarise(Rating_Count = n()) %>%
   ggplot(aes(Rating_Count)) +
   stat_ecdf(geom = "step",
             color = "darkblue") +
   geom_vline(xintercept = quantile_user,
              col = "red",
              linetype = "dashed") +
   labs(title = "Cumulative Distribution ECDF Of Ratings Per User (edx)",
        x = "Users",
        y = "Cumulative Proportion Of Ratings") +
   theme_economist() +
   scale_color_economist() +
   theme(plot.title = element_text(size = 11, color = "darkblue", hjust = 0.5))

```

The figure _"Cumulative Distribution ECDF Of Ratings Per User (edx)"_ shows that only **_`r quantile_user`_** distinct users give **_`90%`_** of the ratings.

\newpage

**_What are the top 10 Users/Raters by frequency?_**

```{r 007-08-WhatAreTheTop10UsersRatersByFrequency}

# 007-08 What are the top 10 Users/Raters by frequency?
edx %>%
   group_by(userId) %>%
   summarise(Rating_Count = n(),
             Rating_Average = round(mean(rating), round_precision)) %>%
   arrange(desc(Rating_Count)) %>%
   head(10) %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2:3, bold = TRUE)

```

The table _"Top 10 Users/Raters By frequency (edx)"_ shows that some user are more active rating the movies than others.


**_What are the top 10 Users/Raters with at least 100 ratings?_**

```{r 007-09-WhatAreTheTop10UsersRatersWithAtLeast100Ratings}

# 007-09 What are the top 10 users/raters with at least 100 ratings?
edx %>%
   group_by(userId) %>%
   summarise(Rating_Count = n(),
             Rating_Average = round(mean(rating), round_precision)) %>%
   filter(Rating_Count >= 100) %>%
   arrange(desc(Rating_Average)) %>%
   head(10) %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2:3, bold = TRUE)

```

The table above shows that the top 10 users / raters are very active. However, there is a substantial difference in the numbers of ratings that a specific user rates.

\newpage

### Genre Analysis


**_What is the genre ratings distribution?_**

```{r 007-10-WhatIsTheGenreRatingsDistribution}

# 007-10 What is the genre ratings distribution?
movieGenre_edx %>%
   ggplot(aes(x = reorder(movieGenre, Movie_Distinct_Count),
              y = Movie_Distinct_Count)) +
   geom_bar(stat = "identity",
            color = "darkblue",
            fill = "lightblue") +
   coord_flip() +
   labs(title = "Movie Genre Distribution (edx)",
        x = "Genre",
        y = "Count Distinct Movies") +
   theme_economist() +
   scale_color_economist() +
   theme(plot.title = element_text(size = 11, color = "darkblue", hjust = 0.5))

```

The table above shows that the most common and popular genres are `Drama` and `Comedy`.


**_What is the average ratings by genre?_**

```{r 007-11-WhatIsTheAverageRatingsByGenre}

# 007-11 What is the average ratings by genre?
movieGenre_edx %>%
   ggplot(aes(x = reorder(movieGenre, Rating_Average),
              y = Rating_Average)) +
   geom_bar(stat = "identity",
            color = "darkblue",
            fill = "lightblue") +
   coord_flip() +
   labs(title = "Average Rating Distribution By Genre (edx)",
        x = "Genre",
        y = "Rating") +
   theme_economist() +
   scale_color_economist() +
   theme(plot.title = element_text(size = 11, color = "darkblue", hjust = 0.5))

```

The table above shows that the average rating by genre is between **_`r round(min(movieGenre_edx$Rating_Average), 2)`_** stars and **_`r round(max(movieGenre_edx$Rating_Average), 2)`_** stars.

\newpage

**_What is the number of ratings by genre?_**

```{r 007-12-WhatIsTheNumberOfRatingsByGenre}

# 007-12 What is the number of ratings by genre?
movieGenre_edx %>%
   ggplot(aes(x = reorder(movieGenre, Rating_Count),
              y = Rating_Count)) +
   geom_bar(stat = "identity",
            color = "darkblue",
            fill = "lightblue") +
   coord_flip() +
   labs(title = "Number Of Rating Distribution By Genre (edx)",
        x = "Genre",
        y = "Number Of Rating") +
   theme_economist() +
   scale_color_economist() +
   theme(plot.title = element_text(size = 11, color = "darkblue", hjust = 0.5))

```

The table above shows that there is a substantial difference in the numbers of ratings that a specific genre receives. Definetely, there is a `genre` effect.

\newpage

## Model Building, Training and Validation


### Key steps

After an in-depth Exploratory Data Analysis, we are ready to build, train and test different algorithms and models to reach our goal that provide a minimal reported RMSE (Root Mean Square Error) on the validation dataset with **_`RMSE lower or equal to 0.87750`_**.

To build and train the different models, we will proceed, for each model, in five sequential steps:

1. Define and Build the model
2. Train the algorithm in the training dataset `edx`
3. Tune the algorithm using techniques such as Regularisation (optimizing `lambda`)
4. Validate the algorithm by runing the predictions in the validation dataset `validation` and comparing the predicted RMSE against our goal
5. Iterate over the models until goal satisfaction

Based on our Exploratory Data Analysis, we decided to test the following models:

- Model 1: Model-based approach (Naive Baseline)
- Model 2: Content-based approach (Movie Effects)
- Model 3: User-based approach (Movie Effects + User Effects)
- Model 4: Regularized Content-based approach (Movie Effects + Regularisation)
- Model 5: Regularized User-based approach (Movie Effects + User Effects + Regularisation)

```{r 008-CreateRMSEFunction, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# =============================================================================
#     Step 008 - Create a function to compute the Residual Mean Squared Error
#                 (RMSE) known as ("typical error")
#
# =============================================================================
# Chapter 34.7.3 Loss function
# We write a loss-function that computes the Residual Mean Squared Error ("typical error") as
# our measure of accuracy. The value is the typical error in star rating we would make
RMSE_fct <- function(actual_ratings, predicted_ratings){
   sqrt(mean((actual_ratings - predicted_ratings)^2, na.rm = TRUE))
}

```


### Model 1 - Model-based approach (Naive Baseline)

This model is the simplest possible recommendation algorithm. The model predicts the same rating for all movies regardless of other features.

$$Y_{u,i} = \mu + \varepsilon_{u,i}$$

With $\varepsilon_{i,u}$ independent errors sampled from the same distribution centered at 0 and $\mu$ the “true” rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of $\mu$ and, in this case, is the average of all ratings.

```{r 009-01-Model-1}

# =============================================================================
#     Step 009.01 - Model Building, Training and Validation
#                 Model 1 - Model-based approach (Naive Baseline)
# =============================================================================

# Identify the Model
model_1_id <- "Model 1"
model_1_desc <- "Model-based approach (Naive Baseline)"

# Calculate the average of all movies
mu_hat <- mean(edx$rating)

# Predict the RMSE on the validation set
RMSE_model_1 <- round(RMSE_fct(validation$rating, mu_hat), round_precision)

# We generate a table to record our approaches and the RMSE for the model
rmse_result_model_1 <- tibble(Model_Id = model_1_id,
                              Model_Method = model_1_desc,
                              Predicted_RMSE = RMSE_model_1)

# We generate a table to record our approaches and the RMSEs we generate.
rmse_results <- rmse_result_model_1

# Display the RMSE results
rmse_result_model_1 %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2, width = "20em") %>%
   column_spec(3, bold = TRUE)

```

The predicted RMSE on the `validation` dataset for the **_`r model_1_desc`_** is about **_`r RMSE_model_1`_** which is far from our goal **_`r RMSE_target_goal`_**. We definetly can perform better prediction.

\newpage

### Model 2 - Content-based approach (Movie Effects)

We know from experience that some movies are just generally rated higher than others. The Exploratory Data Analysis performed on the movies confirm that different movies are rated differently. We can add a new term $b_i$ (item / movie feature) to our previous model 1 to represent average ranking for movie $i$

$$Y_{u,i} = \mu + b_i + \varepsilon_{u,i}$$

With $b_i$ the bias effect for the movie $i$.

```{r 009-02-Model-2}

# =============================================================================
#     Step 009.02 - Model Building, Training and Validation
#                 Model 2 - Content-based approach (Movie Effects)
# =============================================================================

# Identify the Model
model_2_id <- "Model 2"
model_2_desc <- "Content-based approach (Movie Effects)"

# Calculate the average of all movies
mu_hat <- mean(edx$rating)

# Calculate the average by movie
movie_avgs <- edx %>%
   group_by(movieId) %>%
   summarize(b_i = mean(rating - mu_hat))

# Compute the predicted ratings on validation dataset
predicted_ratings <- validation %>%
   left_join(movie_avgs, by='movieId') %>%
   mutate(pred = mu_hat + b_i) %>%
   pull(pred)

# Predict the RMSE on the validation set
RMSE_model_2 <- round(RMSE_fct(predicted_ratings, validation$rating), round_precision)

# We generate a table to record our approaches and the RMSE for the model
rmse_result_model_2 <- tibble(Model_Id = model_2_id,
                              Model_Method = model_2_desc,
                              Predicted_RMSE = RMSE_model_2)

# We generate a table to record our approaches and the RMSEs we generate.
rmse_results <- bind_rows(rmse_results,
                          rmse_result_model_2)

# Display the RMSE results
rmse_result_model_2 %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2, width = "20em") %>%
   column_spec(3, bold = TRUE)

```

The predicted RMSE on the `validation` dataset for the **_`r model_2_desc`_** is about **_`r RMSE_model_2`_** which is better than the **_`r model_1_id`_**. We still do not reach our goal **_`r RMSE_target_goal`_**. We can perform better prediction.


### Model 3 - User-based approach (Movie Effects + User Effects)

From our Exploratory Data Analysis performed on the users, we know that there is a substantial variability across the users. This implies that we can improve our previous model. We can add a new term $b_u$ (user feature) to our previous model 2 to represent the average ranking for users $u$

$$Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}$$

With $b_u$ the bias effect for the user $u$.

```{r 009-03-Model-3}

# =============================================================================
#     Step 009.03 - Model Building, Training and Validation
#                 Model 3 - User-based approach (Movie Effects + User Effects)
# =============================================================================

# Identify the Model
model_3_id <- "Model 3"
model_3_desc <- "User-based approach (Movie Effects + User Effects)"

# Calculate the average of all movies
mu_hat <- mean(edx$rating)

# Calculate the average by movie
movie_avgs <- edx %>%
   group_by(movieId) %>%
   summarize(b_i = mean(rating - mu_hat))

# Calculate the average by user
user_avgs <- edx %>%
   left_join(movie_avgs, by='movieId') %>%
   group_by(userId) %>%
   summarize(b_u = mean(rating - mu_hat - b_i))

# Compute the predicted ratings on validation dataset
predicted_ratings <- validation %>%
   left_join(movie_avgs, by='movieId') %>%
   left_join(user_avgs, by='userId') %>%
   mutate(pred = mu_hat + b_i + b_u) %>%
   pull(pred)

# Predict the RMSE on the validation set
RMSE_model_3 <- round(RMSE_fct(predicted_ratings, validation$rating), round_precision)

# We generate a table to record our approaches and the RMSE for the model
rmse_result_model_3 <- tibble(Model_Id = model_3_id,
                              Model_Method = model_3_desc,
                              Predicted_RMSE = RMSE_model_3)

# We generate a table to record our approaches and the RMSEs they generate.
rmse_results <- bind_rows(rmse_results,
                          rmse_result_model_3)

# Display the RMSE results
rmse_result_model_3 %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2, width = "20em") %>%
   column_spec(3, bold = TRUE)

```

The predicted RMSE on the `validation` dataset for the **_`r model_3_desc`_** is about **_`r RMSE_model_3`_** which is better than the **_`r model_2_id`_**. Now, we do reach our goal **_`r RMSE_target_goal`_**. But, could we tune the previous model to perform better prediction?

\newpage

### Model 4 - Regularized Content-based approach (Movie Effects + Regularisation)

We have seen in the EDA section that the supposed “best” and “worst” movies were rated by very few users, in most cases just 1. This is because with just a few users, we have more uncertainty. Therefore, larger estimates of $b_i$, negative or positive, are more likely.

These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.

Regularization permits us to penalize large estimates that are formed using small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions. The general idea of penalized regression is to control the total variability of the movie effects. Specifically, instead of minimizing the least square equation, we minimize an equation that adds a penalty:

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2$$
The first term is just least squares and the second is a penalty that gets larger when many $b_i$ are large. Using calculus we can actually show that the values of $b_i$ that minimize this equation are:

$$\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)$$

where $n_i$ is the number of ratings made for movie $i$. This approach will have our desired effect: when our sample size $n_i$ is very large, a case which will give us a stable estimate, then the penalty $\lambda$ is effectively ignored since $n_i+\lambda \approx n_i$. However, when the $n_i$ is small, then the estimate $\hat{b}_i(\lambda)$ is shrunken towards 0. The larger $\lambda$, the more we shrink.

So, let's apply the regularisation on the **_`r model_2_desc`_**.

```{r 009-04-Model-4}

# =============================================================================
#     Step 009.04 - Model Building, Training and Validation
#                 Model 4 - Regularized Content-based approach
#                          (Movie Effects + Regularisation)
# =============================================================================

# Identify the Model
model_4_id <- "Model 4"
model_4_desc <- "Regularized Content-based approach (Movie Effects + Regularisation)"

# Calculate the average of all movies
mu_hat <- mean(edx$rating)

# Define a table of lambdas
lambdas <- seq(0, 10, 0.1)

# Compute the predicted ratings on validation dataset using different values of lambda
rmses <- sapply(lambdas, function(lambda) {

   # Calculate the average by user
   b_i <- edx %>%
      group_by(movieId) %>%
      summarize(b_i = sum(rating - mu_hat) / (n() + lambda))

   # Compute the predicted ratings on validation dataset
   predicted_ratings <- validation %>%
      left_join(b_i, by='movieId') %>%
      mutate(pred = mu_hat + b_i) %>%
      pull(pred)

   # Predict the RMSE on the validation set
   return(RMSE_fct(predicted_ratings, validation$rating))
})

# plot the result of lambdas (to be commented for the Rmarkdown document)
# qplot(lambdas, rmses)

# Get the lambda value that minimize the RMSE
min_lambda <- lambdas[which.min(rmses)]

# Predict the RMSE on the validation set
RMSE_model_4 <- round(min(rmses), round_precision)

# We generate a table to record our approaches and the RMSE for the model
rmse_result_model_4 <- tibble(Model_Id = model_4_id,
                              Model_Method = model_4_desc,
                              Predicted_RMSE = RMSE_model_4)

# We generate a table to record our approaches and the RMSEs they generate.
rmse_results <- bind_rows(rmse_results,
                          rmse_result_model_4)

# Display the RMSE results
rmse_result_model_4 %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2, width = "20em") %>%
   column_spec(3, bold = TRUE)

```

The predicted RMSE on the `validation` dataset for the **_`r model_4_desc`_** is about **_`r RMSE_model_4`_** which is better than the **_`r model_2_id`_** after applying the regularisation using $\lambda$ equal to **_`r min_lambda`_** that minimize the RMSE. We still do not reach our goal **_`r RMSE_target_goal`_**. We can perform better prediction.

\newpage

### Model 5 - Regularized User-based approach (Movie Effects + User Effects + Regularisation)

Let's apply the same regularisation technique on the **_`r model_3_desc`_**.

```{r 009-05-Model-5}

# =============================================================================
#     Step 009.05 - Model Building, Training and Validation
#                 Model 5 - Regularized User-based approach
#                          (Movie Effects + User Effects + Regularisation)
# =============================================================================

# Identify the Model
model_5_id <- "Model 5"
model_5_desc <- "Regularized User-based approach (Movie Effects + User Effects + Regularisation)"

# Define a table of lambdas
lambdas <- seq(0, 10, 0.1)

# Compute the predicted ratings on validation dataset using different values of lambda
rmses <- sapply(lambdas, function(lambda) {

   # Calculate the average of all movies
   mu_hat <- mean(edx$rating)

   # Calculate the average by user
   b_i <- edx %>%
      group_by(movieId) %>%
      summarize(b_i = sum(rating - mu_hat) / (n() + lambda))

   # Calculate the average by user
   b_u <- edx %>%
      left_join(b_i, by='movieId') %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - b_i - mu_hat) / (n() + lambda))

   # Compute the predicted ratings on validation dataset
   predicted_ratings <- validation %>%
      left_join(b_i, by='movieId') %>%
      left_join(b_u, by='userId') %>%
      mutate(pred = mu_hat + b_i + b_u) %>%
      pull(pred)

   # Predict the RMSE on the validation set
   return(RMSE_fct(predicted_ratings, validation$rating))
})

# plot the result of lambdas (to be commented for the Rmarkdown document)
# qplot(lambdas, rmses)

# Get the lambda value that minimize the RMSE
min_lambda <- lambdas[which.min(rmses)]

# Predict the RMSE on the validation set
RMSE_model_5 <- round(min(rmses), round_precision)

# We generate a table to record our approaches and the RMSE for the model
rmse_result_model_5 <- tibble(Model_Id = model_5_id,
                              Model_Method = model_5_desc,
                              Predicted_RMSE = RMSE_model_5)

# We generate a table to record our approaches and the RMSEs they generate.
rmse_results <- bind_rows(rmse_results,
                          rmse_result_model_5)

# Display the RMSE results
rmse_result_model_5 %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2, width = "20em") %>%
   column_spec(3, bold = TRUE)

```

The predicted RMSE on the `validation` dataset for the **_`r model_5_desc`_** is about **_`r RMSE_model_5`_** which is better than the **_`r model_3_id`_** after applying the regularisation using $\lambda$ equal to **_`r min_lambda`_** that minimize the RMSE. 

Now, we do reach our goal **_`r RMSE_target_goal`_**.

\newpage

<!-- ============================================================================== -->
<!--  Section  03. Results                                                          -->
<!-- ============================================================================== -->
# 3. Results

Here is the summary of the RMSEs After building, training and validating different models on the `validation` dataset:

```{r 010-Results}

# =============================================================================
#     Step 010 - RMSE results on all the models
# =============================================================================

# Display the RMSE results
rmse_results %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped","hoved","condensed"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE) %>%
   column_spec(1, width = "5em") %>%
   column_spec(2, width = "20em") %>%
   column_spec(3, bold = TRUE)

```

We can observe that a better RMSE is obtained from **_`r model_5_desc`_**. We can conclude that this is our definitive model as it is achieving our goal.

We achieved an  **RMSE = `r RMSE_model_5`** which is more than the expected goal of **_Target RMSE = `r RMSE_target_goal`_**.


<!-- ============================================================================== -->
<!--  Section  04. Conclusion                                                       -->
<!-- ============================================================================== -->
# 4. Conclusion

In this project, we have used an iterative applied machine learning approach to implement a movie recommendation system to predict a movie rating by a given user. We have build, train and validate naive approach, movie effects, user effects and tuned the models using regularisation technique to predict the movie ratings by users with a targeted RMSE.

As we have seen the Exploratory Data Analysis, the MovieLens dataset contains some other features that we could use as predictors like `genres`, `releaseYear (from title)` and `ratingYear (from timestamp)`.

We could also build and train other models such as KNN, Kmeans, Random Forest, Matrix Factorization, SVD, PCA, Slope One. Some of these algorithms could improve the RMSEs, others could improve the Accuracy.

The most challenging impediment that we might encounter when implementing those additional models and going further in the overall optimizations are both the dataset size and the machine computation resources.

However, using only two predictors `movieId` and `userId`, we wre able to reach our objective.
